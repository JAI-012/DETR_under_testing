# EdgeDETR: Lightweight Object Detection for Robotics

A custom DETR (Detection Transformer) implementation optimized for NVIDIA Jetson platforms, designed for real-time object detection in robotics applications.

## üéØ Project Overview

This project implements a lightweight object detection model using:
- **Backbone**: MobileNetV3-Large (optimized for edge devices)
- **Architecture**: Transformer-based detection (DETR approach)
- **Target**: Single-class object detection for robotic manipulation
- **Hardware**: NVIDIA Jetson Nano/Xavier or CUDA-enabled GPUs

### Key Features
- ‚úÖ Lightweight architecture suitable for embedded systems
- ‚úÖ Real-time inference capability (~15-30 FPS on Jetson)
- ‚úÖ YOLO-format label compatibility
- ‚úÖ Simple L1 loss for fast convergence
- ‚úÖ Direct camera integration for live detection

---

## üìÅ Project Structure

```
DeepSeek_Robot_Vision/
‚îú‚îÄ‚îÄ dataset.py              # PyTorch Dataset class for loading images and labels
‚îú‚îÄ‚îÄ my_model.py             # EdgeDETR model architecture
‚îú‚îÄ‚îÄ train.py                # Training script with Hungarian matching
‚îú‚îÄ‚îÄ inference.py            # Real-time camera inference script
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îú‚îÄ‚îÄ images/             # Training images (.jpg)
‚îÇ   ‚îú‚îÄ‚îÄ labels/             # YOLO format labels (.txt)
‚îÇ   ‚îî‚îÄ‚îÄ validate.py         # Script to validate label format
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ __init__.py         # Package initialization
    ‚îú‚îÄ‚îÄ box_ops.py          # Bounding box utility functions
    ‚îî‚îÄ‚îÄ matcher.py          # Hungarian matching algorithm
```

---

## üõ†Ô∏è Installation

### Prerequisites
- Python 3.8+
- CUDA-capable GPU (optional but recommended)
- Webcam/camera for inference

### Install Dependencies

```bash
pip install torch torchvision opencv-python scipy
```

**For Jetson Nano** (use pre-built PyTorch wheels):
```bash
# Follow NVIDIA's official guide for PyTorch on Jetson:
# https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform/
```

---

## üìä Dataset Preparation

### Label Format (YOLO)
Each `.txt` file should contain one line per object:
```
class_id center_x center_y width height
```

All values are **normalized** (0-1 range):
- `class_id`: Integer class ID (0 for single-class detection)
- `center_x`, `center_y`: Bounding box center coordinates
- `width`, `height`: Bounding box dimensions

**Example** (`labels/image001.txt`):
```
0 0.5 0.5 0.3 0.2
```

### Dataset Structure
```
dataset/
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ image001.jpg
‚îÇ   ‚îú‚îÄ‚îÄ image002.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ labels/
    ‚îú‚îÄ‚îÄ image001.txt
    ‚îú‚îÄ‚îÄ image002.txt
    ‚îî‚îÄ‚îÄ ...
```

### Validate Your Dataset
Before training, verify your labels are correctly formatted:

```bash
cd dataset
python validate.py
```

**Expected output:**
```
‚úÖ All labels look good!
```

**If issues found:**
```
‚ùå PROBLEMS FOUND:
  - image001.txt line 1: Values not normalized! cx=320 cy=240 w=100 h=80
```
‚Üí Fix labels to use 0-1 normalized values.

---

## üöÄ Training

### Basic Training
```bash
python train.py
```

### Training Configuration
Edit `train.py` to modify hyperparameters:

```python
# Model Configuration
model = EdgeDETR(num_classes=1)  # Change if multi-class

# Training Settings
batch_size = 4        # Reduce to 2 if OOM errors
epochs = 50          # Increase to 100 for better accuracy
learning_rate = 1e-4  # Reduce to 1e-5 if loss becomes NaN
```

### Expected Training Output
```
Starting Training...
Epoch 0: Loss 0.3452
Epoch 1: Loss 0.2891
Epoch 2: Loss 0.2453
...
Epoch 49: Loss 0.0234
Training Done. 'weights.pth' saved.
```

### Training Time Estimates
- **GPU (GTX 1060+)**: 10-30 minutes for 50 epochs
- **CPU**: 2-4 hours for 50 epochs
- **Jetson Nano**: Not recommended (use trained weights instead)

---

## üé• Inference (Real-Time Detection)

### Run Camera Inference
```bash
python inference.py
```

### Controls
- **'q' key**: Quit the application
- Green boxes indicate detected objects with >70% confidence

### Modify Detection Settings

**Change confidence threshold:**
```python
# In inference.py (line 32)
keep = probs.max(-1).values > 0.5  # Lower to 0.5 for more detections
```

**Change camera source:**
```python
# In inference.py (line 16)
cap = cv2.VideoCapture(0)  # Change 0 to 1, 2, etc. for different cameras
```

### Output Format
The inference script provides bounding box coordinates in pixels:
```python
cx, cy, bw, bh = box * [w, h, w, h]
# cx, cy: center coordinates (pixels)
# bw, bh: box width/height (pixels)
```

**Integration with robotics:**
```python
# Uncomment in inference.py to enable robot control
real_x, real_y = pixel_to_cm(cx, cy)  # Your camera calibration function
move_robot(real_x, real_y)             # Your robot control function
```

---

## üîß Troubleshooting

### Common Errors

#### 1. **ImportError: No module named 'utils'**
**Solution:** Ensure `utils/__init__.py` exists (can be empty).

#### 2. **CUDA Out of Memory**
**Solution:** Reduce batch size in `train.py`:
```python
loader = DataLoader(ds, batch_size=2, ...)  # Reduce from 4 to 2
```

#### 3. **Loss becomes NaN**
**Solution:** Reduce learning rate:
```python
optim = torch.optim.AdamW(model.parameters(), lr=1e-5)  # Reduce from 1e-4
```

#### 4. **No detections during inference**
**Possible causes:**
- Model not trained enough (increase epochs)
- Labels were incorrect during training (run validate.py)
- Confidence threshold too high (lower from 0.7 to 0.5)

**Solution:** Lower threshold temporarily to test:
```python
keep = probs.max(-1).values > 0.3  # Test with low threshold
```

#### 5. **ValueError: not enough values to unpack**
**Cause:** Label file has wrong format (not 5 values per line)

**Solution:** Run `dataset/validate.py` and fix problematic labels.

---

## üìà Model Performance

### Expected Accuracy
- **100-200 images**: 70-85% detection rate
- **500+ images**: 85-95% detection rate
- **Inference speed**: 15-30 FPS on Jetson Nano

### Improving Accuracy
1. **Collect more data** (aim for 200+ images)
2. **Increase training epochs** (50 ‚Üí 100)
3. **Lower confidence threshold** (0.7 ‚Üí 0.5)
4. **Verify label quality** with validate.py

---

## üî¨ Model Architecture Details

### EdgeDETR Components

1. **Backbone (MobileNetV3)**
   - Input: 3√óH√óW RGB image
   - Output: 960√óH/32√óW/32 feature map
   - Purpose: Extract visual features efficiently

2. **Transformer**
   - 4 encoder layers + 4 decoder layers
   - 8 attention heads
   - 256 hidden dimensions
   - 100 object queries (predictions)

3. **Prediction Heads**
   - Classification: Predicts object class + background
   - Bounding Box: Predicts [cx, cy, w, h] in 0-1 range

### Training Strategy
- **Matcher**: Hungarian algorithm matches predictions to ground truth
- **Loss**: L1 distance between matched boxes (simplified DETR loss)
- **Optimizer**: AdamW with 1e-4 learning rate

---

## üìù Citation

This implementation is inspired by:
- **DETR**: End-to-End Object Detection with Transformers (Facebook AI Research)
- **MobileNetV3**: Searching for MobileNetV3 (Google Research)

---

## ü§ù Contributing

This is a student robotics project. Feedback and improvements are welcome!

### Known Limitations
- Single-class detection only (extend `num_classes` for multi-class)
- Simplified loss function (L1 only, no classification/GIoU loss)
- No validation split during training
- Fixed input size (model adapts but may affect accuracy)

---

## üìß Support

If you encounter issues:
1. Run `dataset/validate.py` to check labels
2. Verify all files are `.py` (not `.txt`)
3. Ensure `utils/__init__.py` exists
4. Check GPU memory usage (reduce batch size if needed)

---

## üéì Educational Notes

### Why This Architecture?
- **MobileNetV3**: Efficient for Jetson (low power consumption)
- **Transformer**: Better than CNN-only for small objects
- **Hungarian Matching**: Optimal assignment between predictions and labels
- **L1 Loss**: Simple but effective for fixed-camera scenarios

### Recommended Reading
- [DETR Paper](https://arxiv.org/abs/2005.12872)
- [MobileNetV3 Paper](https://arxiv.org/abs/1905.02244)
- [Hungarian Algorithm Explained](https://en.wikipedia.org/wiki/Hungarian_algorithm)

---

**License:** MIT (Educational Use)  
**Version:** 1.0.0  
**Last Updated:** January 2026